{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-15T08:31:57.482423Z",
     "start_time": "2025-10-15T08:31:36.323218Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "class TestScraper:\n",
    "    \"\"\"\n",
    "    测试网站爬虫类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "\n",
    "    def test_httpbin(self):\n",
    "        \"\"\"测试httpbin.org\"\"\"\n",
    "        print(\"=== 测试 httpbin.org ===\")\n",
    "\n",
    "        tests = {\n",
    "            'JSON响应': 'https://httpbin.org/json',\n",
    "            'HTML响应': 'https://httpbin.org/html',\n",
    "            '请求头信息': 'https://httpbin.org/headers',\n",
    "            'IP信息': 'https://httpbin.org/ip'\n",
    "        }\n",
    "\n",
    "        results = []\n",
    "        for name, url in tests.items():\n",
    "            try:\n",
    "                response = self.session.get(url)\n",
    "                data = {\n",
    "                    '测试项目': name,\n",
    "                    'URL': url,\n",
    "                    '状态码': response.status_code,\n",
    "                    '内容类型': response.headers.get('content-type', ''),\n",
    "                    '数据大小': len(response.text)\n",
    "                }\n",
    "                results.append(data)\n",
    "                print(f\"✅ {name}: 成功\")\n",
    "\n",
    "                # 显示部分内容\n",
    "                if name == 'JSON响应':\n",
    "                    print(f\"   示例数据: {response.json()[:100]}...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {name}: 失败 - {e}\")\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def test_quotes(self):\n",
    "        \"\"\"测试名言网站\"\"\"\n",
    "        print(\"\\n=== 测试 quotes.toscrape.com ===\")\n",
    "\n",
    "        base_url = 'http://quotes.toscrape.com'\n",
    "        quotes_data = []\n",
    "\n",
    "        try:\n",
    "            # 爬取第一页\n",
    "            response = self.session.get(base_url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            quotes = soup.find_all('div', class_='quote')\n",
    "            for quote in quotes:\n",
    "                text = quote.find('span', class_='text').get_text()\n",
    "                author = quote.find('small', class_='author').get_text()\n",
    "                tags = [tag.get_text() for tag in quote.find_all('a', class_='tag')]\n",
    "\n",
    "                quotes_data.append({\n",
    "                    '名言': text,\n",
    "                    '作者': author,\n",
    "                    '标签': ', '.join(tags)\n",
    "                })\n",
    "\n",
    "            print(f\"✅ 成功提取 {len(quotes_data)} 条名言\")\n",
    "\n",
    "            # 尝试翻页\n",
    "            next_button = soup.find('li', class_='next')\n",
    "            if next_button:\n",
    "                next_page = next_button.find('a')['href']\n",
    "                print(f\"   发现下一页: {next_page}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 爬取失败: {e}\")\n",
    "\n",
    "        return pd.DataFrame(quotes_data)\n",
    "\n",
    "    def test_books(self):\n",
    "        \"\"\"测试图书网站（只爬取第一页作为示例）\"\"\"\n",
    "        print(\"\\n=== 测试 books.toscrape.com ===\")\n",
    "\n",
    "        base_url = 'http://books.toscrape.com'\n",
    "        books_data = []\n",
    "\n",
    "        try:\n",
    "            response = self.session.get(base_url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            books = soup.find_all('article', class_='product_pod')\n",
    "            for book in books:\n",
    "                title = book.find('h3').find('a')['title']\n",
    "                price = book.find('p', class_='price_color').get_text()\n",
    "                availability = book.find('p', class_='instock').get_text().strip()\n",
    "\n",
    "                books_data.append({\n",
    "                    '书名': title,\n",
    "                    '价格': price,\n",
    "                    '库存状态': availability\n",
    "                })\n",
    "\n",
    "            print(f\"✅ 成功提取 {len(books_data)} 本图书信息\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 爬取失败: {e}\")\n",
    "\n",
    "        return pd.DataFrame(books_data)\n",
    "\n",
    "    def test_api_data(self):\n",
    "        \"\"\"测试API数据获取\"\"\"\n",
    "        print(\"\\n=== 测试 API 数据获取 ===\")\n",
    "\n",
    "        try:\n",
    "            # 测试假商店API\n",
    "            response = self.session.get('https://fakestoreapi.com/products')\n",
    "            products = response.json()\n",
    "\n",
    "            api_data = []\n",
    "            for product in products[:5]:  # 只取前5个作为示例\n",
    "                api_data.append({\n",
    "                    '商品ID': product['id'],\n",
    "                    '标题': product['title'],\n",
    "                    '价格': product['price'],\n",
    "                    '分类': product['category']\n",
    "                })\n",
    "\n",
    "            print(f\"✅ 成功获取 {len(products)} 个商品数据（显示前5个）\")\n",
    "            return pd.DataFrame(api_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ API测试失败: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    \"\"\"主测试函数\"\"\"\n",
    "    scraper = TestScraper()\n",
    "\n",
    "    # 执行各项测试\n",
    "    httpbin_results = scraper.test_httpbin()\n",
    "    quotes_results = scraper.test_quotes()\n",
    "    books_results = scraper.test_books()\n",
    "    api_results = scraper.test_api_data()\n",
    "\n",
    "    # 显示结果摘要\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"测试结果摘要:\")\n",
    "    print(f\"HTTPBIN测试: {len(httpbin_results)} 项\")\n",
    "    print(f\"名言采集: {len(quotes_results)} 条\")\n",
    "    print(f\"图书采集: {len(books_results)} 本\")\n",
    "    print(f\"API数据: {len(api_results)} 条\")\n",
    "\n",
    "    # 保存结果（可选）\n",
    "    if not quotes_results.empty:\n",
    "        quotes_results.to_csv('quotes_data.csv', index=False, encoding='utf-8-sig')\n",
    "        print(\"名言数据已保存为 quotes_data.csv\")\n",
    "\n",
    "    if not books_results.empty:\n",
    "        books_results.to_csv('books_data.csv', index=False, encoding='utf-8-sig')\n",
    "        print(\"图书数据已保存为 books_data.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 测试 httpbin.org ===\n",
      "✅ JSON响应: 成功\n",
      "❌ JSON响应: 失败 - unhashable type: 'slice'\n",
      "✅ HTML响应: 成功\n",
      "✅ 请求头信息: 成功\n",
      "✅ IP信息: 成功\n",
      "\n",
      "=== 测试 quotes.toscrape.com ===\n",
      "✅ 成功提取 10 条名言\n",
      "   发现下一页: /page/2/\n",
      "\n",
      "=== 测试 books.toscrape.com ===\n",
      "✅ 成功提取 20 本图书信息\n",
      "\n",
      "=== 测试 API 数据获取 ===\n",
      "✅ 成功获取 20 个商品数据（显示前5个）\n",
      "\n",
      "==================================================\n",
      "测试结果摘要:\n",
      "HTTPBIN测试: 4 项\n",
      "名言采集: 10 条\n",
      "图书采集: 20 本\n",
      "API数据: 5 条\n",
      "名言数据已保存为 quotes_data.csv\n",
      "图书数据已保存为 books_data.csv\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
