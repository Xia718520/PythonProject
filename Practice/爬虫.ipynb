{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:21:30.524142Z",
     "start_time": "2025-10-15T08:21:29.869962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "def get_web_info(url):\n",
    "    \"\"\"\n",
    "    获取在线网页信息的通用函数\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 向目标网页发出请求\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # 检查请求状态\n",
    "        print(f\"状态码: {response.status_code}\")\n",
    "\n",
    "        # 自动检测并设置编码\n",
    "        response.encoding = response.apparent_encoding\n",
    "\n",
    "        # 返回网页内容\n",
    "        return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"请求失败: {e}\")\n",
    "        return None\n",
    "\n",
    "# 使用示例\n",
    "url = 'http://www.weather.com.cn/'\n",
    "html_text = get_web_info(url)\n",
    "\n",
    "if html_text:\n",
    "    print(\"网页获取成功！\")\n",
    "    print(f\"网页内容长度: {len(html_text)} 字符\")\n",
    "    # 打印前500个字符预览\n",
    "    print(html_text[:500])\n",
    "else:\n",
    "    print(\"网页获取失败！\")"
   ],
   "id": "b50ad85ca570a53b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态码: 200\n",
      "网页获取成功！\n",
      "网页内容长度: 139689 字符\n",
      "<!DOCTYPE html>\r\n",
      "<html>\r\n",
      "<head>\r\n",
      "    <link rel=\"dns-prefetch\" href=\"https://i.tq121.com.cn\" />\r\n",
      "    <meta charset=\"utf-8\" />\r\n",
      "    <title>天气网</title>\r\n",
      "    <link rel=\"icon\" href=\"https://www.weather.com.cn/m2/i/favicon.ico?v=3\" type=\"image/x-icon\" />\r\n",
      "    <meta http-equiv=\"Content-Security-Policy\" content=\"default-src 'self' https://*.weather.com.cn; script-src 'self' http://*.tq121.com.cn http://*.i8tq.com http://*.weather.com.cn http://*.baidu.com http://3gimg.qq.com   'unsafe-inline' 'unsafe-ev\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:22:31.173181Z",
     "start_time": "2025-10-15T08:22:26.052256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_web_info_enhanced(url, timeout=10, headers=None):\n",
    "    \"\"\"\n",
    "    增强版网页信息获取函数\n",
    "    \"\"\"\n",
    "    # 默认请求头，模拟浏览器访问\n",
    "    if headers is None:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()  # 如果状态码不是200，抛出异常\n",
    "\n",
    "        response.encoding = response.apparent_encoding\n",
    "        print(f\"✅ 成功获取网页: {url}\")\n",
    "        print(f\"   状态码: {response.status_code}\")\n",
    "        print(f\"   编码: {response.encoding}\")\n",
    "        print(f\"   内容长度: {len(response.text)} 字符\")\n",
    "\n",
    "        return response.text\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ 请求失败: {e}\")\n",
    "        return None\n",
    "\n",
    "# 测试增强版函数\n",
    "test_url = 'https://httpbin.org/json'\n",
    "html_content = get_web_info_enhanced(test_url)"
   ],
   "id": "9b3f98e2de9a1688",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功获取网页: https://httpbin.org/json\n",
      "   状态码: 200\n",
      "   编码: ascii\n",
      "   内容长度: 429 字符\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:26:16.223701Z",
     "start_time": "2025-10-15T08:26:16.219809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_local_info(file_path):\n",
    "    \"\"\"\n",
    "    获取本地网页信息的函数\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 打开指定的本地网页文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "\n",
    "        print(f\"✅ 成功读取本地文件: {file_path}\")\n",
    "        print(f\"   文件内容长度: {len(html_content)} 字符\")\n",
    "\n",
    "        return html_content\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 文件未找到: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 读取文件失败: {e}\")\n",
    "        return None\n",
    "\n",
    "# 使用示例\n",
    "local_file = 'D:/PythonProject/Practice/html/index.html'\n",
    "local_html = get_local_info(local_file)\n",
    "\n",
    "if local_html:\n",
    "    print(\"本地文件内容预览:\")\n",
    "    print(local_html)  # 预览"
   ],
   "id": "c2785d24bf6c6828",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功读取本地文件: D:/PythonProject/Practice/html/index.html\n",
      "   文件内容长度: 532 字符\n",
      "本地文件内容预览:\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "\t<head>\n",
      "\t\t<meta charset=\"utf-8\">\n",
      "\t\t<title>网页中的黄鹤楼</title>\n",
      "\t</head>\n",
      "\t<body style=\"text-align:center;\">\n",
      "\t\t<div >\n",
      "\t\t\t<h2>黄鹤楼风采</h2></br>\n",
      "\t\t\t<p> <img src=\"img1.jpg\"> </p></br>\n",
      "\t\t\t<h3>黄鹤楼实习合影</h3></br>\n",
      "\t\t\t<p> <img src=\"img2.jpg\"></p></br>\n",
      "\t\t\t<h4>《黄鹤楼送孟浩然之广陵》</br> 李白</h4>\n",
      "\t\t\t<p>故人西辞黄鹤楼，烟花三月下扬州。孤帆远影碧空尽，唯见长江天际流。</p></br>\n",
      "\t\t\t<p> <img src=\"img3.jpg\"></p>\n",
      "\t\t\t<h4>《黄鹤楼》崔颢</h4></br>\n",
      "\t\t\t<p> <img src=\"img4.jpg\"></p>\n",
      "\t\t\t<p>\n",
      "\t\t\t\t<audio controls>\n",
      "\t\t\t\t\t<source src=\"music.mp3\" >\n",
      "\t\t\t\t</audio>\n",
      "\t\t\t</p>\n",
      "\t\t</div>\n",
      "\t</body>\n",
      "</html>\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:27:39.694471Z",
     "start_time": "2025-10-15T08:27:39.583527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def prettify_html(html_text):\n",
    "    \"\"\"\n",
    "    结构化网页内容的函数\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 将网页信息导入BeautifulSoup\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "        # 结构化网页信息\n",
    "        prettified_doc = soup.prettify()\n",
    "\n",
    "        print(\"✅ 网页内容结构化完成\")\n",
    "        return prettified_doc, soup\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 结构化失败: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# 使用本地HTML文件进行测试\n",
    "if local_html:\n",
    "    pretty_html, soup_object = prettify_html(local_html)\n",
    "\n",
    "    if pretty_html:\n",
    "        print(\"结构化后的网页内容预览:\")\n",
    "        print(pretty_html[:500])  # 预览前500个字符"
   ],
   "id": "45bf687743f3275a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 网页内容结构化完成\n",
      "结构化后的网页内容预览:\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   网页中的黄鹤楼\n",
      "  </title>\n",
      " </head>\n",
      " <body style=\"text-align:center;\">\n",
      "  <div>\n",
      "   <h2>\n",
      "    黄鹤楼风采\n",
      "   </h2>\n",
      "   <p>\n",
      "    <img src=\"img1.jpg\"/>\n",
      "   </p>\n",
      "   <h3>\n",
      "    黄鹤楼实习合影\n",
      "   </h3>\n",
      "   <p>\n",
      "    <img src=\"img2.jpg\"/>\n",
      "   </p>\n",
      "   <h4>\n",
      "    《黄鹤楼送孟浩然之广陵》\n",
      "    李白\n",
      "   </h4>\n",
      "   <p>\n",
      "    故人西辞黄鹤楼，烟花三月下扬州。孤帆远影碧空尽，唯见长江天际流。\n",
      "   </p>\n",
      "   <p>\n",
      "    <img src=\"img3.jpg\"/>\n",
      "   </p>\n",
      "   <h4>\n",
      "    《黄鹤楼》崔颢\n",
      "   </h4>\n",
      "   <p>\n",
      "    <img src=\"img4.jpg\"/>\n",
      "   </p>\n",
      "   <p>\n",
      "    <audio\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:28:33.586612Z",
     "start_time": "2025-10-15T08:28:33.580080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_hhl_info(html_content):\n",
    "    \"\"\"\n",
    "    提取黄鹤楼网页特定信息的函数\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    print(\"=== 黄鹤楼网页信息提取结果 ===\\n\")\n",
    "\n",
    "    # 1. 提取所有标题\n",
    "    print(\"1. 网页中的标题:\")\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3', 'h4'])\n",
    "    for heading in headings:\n",
    "        print(f\"   - {heading.name}: {heading.get_text().strip()}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # 2. 提取所有段落文本\n",
    "    print(\"2. 网页中的诗歌内容:\")\n",
    "    paragraphs = soup.find_all('p')\n",
    "    for i, p in enumerate(paragraphs, 1):\n",
    "        text = p.get_text().strip()\n",
    "        if text and not p.find('img') and not p.find('audio'):\n",
    "            print(f\"   {i}. {text}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # 3. 提取所有图片\n",
    "    print(\"3. 网页中的图片:\")\n",
    "    images = soup.find_all('img')\n",
    "    for i, img in enumerate(images, 1):\n",
    "        src = img.get('src', '无src属性')\n",
    "        print(f\"   图片{i}: {src}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # 4. 提取多媒体内容\n",
    "    print(\"4. 网页中的多媒体:\")\n",
    "    audio_elements = soup.find_all('audio')\n",
    "    for i, audio in enumerate(audio_elements, 1):\n",
    "        source = audio.find('source')\n",
    "        if source:\n",
    "            src = source.get('src', '无音频源')\n",
    "            print(f\"   音频{i}: {src}\")\n",
    "\n",
    "    return {\n",
    "        'headings': headings,\n",
    "        'paragraphs': paragraphs,\n",
    "        'images': images,\n",
    "        'audio': audio_elements\n",
    "    }\n",
    "\n",
    "# 执行信息提取\n",
    "if local_html:\n",
    "    extracted_data = extract_hhl_info(local_html)"
   ],
   "id": "554c2b35a0ef5ab5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 黄鹤楼网页信息提取结果 ===\n",
      "\n",
      "1. 网页中的标题:\n",
      "   - h2: 黄鹤楼风采\n",
      "   - h3: 黄鹤楼实习合影\n",
      "   - h4: 《黄鹤楼送孟浩然之广陵》 李白\n",
      "   - h4: 《黄鹤楼》崔颢\n",
      "\n",
      "==================================================\n",
      "\n",
      "2. 网页中的诗歌内容:\n",
      "   3. 故人西辞黄鹤楼，烟花三月下扬州。孤帆远影碧空尽，唯见长江天际流。\n",
      "\n",
      "==================================================\n",
      "\n",
      "3. 网页中的图片:\n",
      "   图片1: img1.jpg\n",
      "   图片2: img2.jpg\n",
      "   图片3: img3.jpg\n",
      "   图片4: img4.jpg\n",
      "\n",
      "==================================================\n",
      "\n",
      "4. 网页中的多媒体:\n",
      "   音频1: music.mp3\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:29:43.164569Z",
     "start_time": "2025-10-15T08:29:41.575884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WebScraper:\n",
    "    \"\"\"\n",
    "    通用网页采集分析类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        # 设置默认请求头\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "\n",
    "    def get_web_content(self, url, is_local=False):\n",
    "        \"\"\"\n",
    "        获取网页内容\n",
    "        \"\"\"\n",
    "        if is_local:\n",
    "            return get_local_info(url)\n",
    "        else:\n",
    "            return get_web_info_enhanced(url)\n",
    "\n",
    "    def analyze_page(self, html_content):\n",
    "        \"\"\"\n",
    "        分析网页结构\n",
    "        \"\"\"\n",
    "        if not html_content:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        analysis = {\n",
    "            'title': soup.title.string if soup.title else '无标题',\n",
    "            'headings': {},\n",
    "            'links': [],\n",
    "            'images': [],\n",
    "            'text_length': len(soup.get_text()),\n",
    "            'tag_count': len(soup.find_all())\n",
    "        }\n",
    "\n",
    "        # 统计各级标题\n",
    "        for i in range(1, 7):\n",
    "            headings = soup.find_all(f'h{i}')\n",
    "            analysis['headings'][f'h{i}'] = len(headings)\n",
    "\n",
    "        # 提取链接\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links[:10]:  # 只取前10个链接\n",
    "            analysis['links'].append({\n",
    "                'text': link.get_text(strip=True),\n",
    "                'url': link['href']\n",
    "            })\n",
    "\n",
    "        # 提取图片\n",
    "        images = soup.find_all('img')\n",
    "        for img in images[:5]:  # 只取前5个图片\n",
    "            analysis['images'].append(img.get('src', '无src'))\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def generate_report(self, analysis):\n",
    "        \"\"\"\n",
    "        生成分析报告\n",
    "        \"\"\"\n",
    "        if not analysis:\n",
    "            return \"无分析数据\"\n",
    "\n",
    "        report = []\n",
    "        report.append(\"=== 网页分析报告 ===\")\n",
    "        report.append(f\"网页标题: {analysis['title']}\")\n",
    "        report.append(f\"文本总长度: {analysis['text_length']} 字符\")\n",
    "        report.append(f\"HTML标签总数: {analysis['tag_count']} 个\")\n",
    "\n",
    "        report.append(\"\\n--- 标题统计 ---\")\n",
    "        for level, count in analysis['headings'].items():\n",
    "            report.append(f\"{level.upper()}: {count} 个\")\n",
    "\n",
    "        report.append(\"\\n--- 前5个链接 ---\")\n",
    "        for i, link in enumerate(analysis['links'][:5], 1):\n",
    "            report.append(f\"{i}. {link['text']} -> {link['url']}\")\n",
    "\n",
    "        report.append(\"\\n--- 图片资源 ---\")\n",
    "        for i, img in enumerate(analysis['images'][:3], 1):\n",
    "            report.append(f\"{i}. {img}\")\n",
    "\n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# 使用示例\n",
    "def main():\n",
    "    scraper = WebScraper()\n",
    "\n",
    "    # # 分析本地网页\n",
    "    # print(\"正在分析本地网页...\")\n",
    "    # local_content = scraper.get_web_content('html/index.html', is_local=True)\n",
    "    # local_analysis = scraper.analyze_page(local_content)\n",
    "\n",
    "    # if local_analysis:\n",
    "    #     report = scraper.generate_report(local_analysis)\n",
    "    #     print(report)\n",
    "\n",
    "    # print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "    # 尝试分析在线网页（可选）\n",
    "    try:\n",
    "        print(\"正在分析在线网页（示例）...\")\n",
    "        online_content = scraper.get_web_content('https://books.toscrape.com/')\n",
    "        online_analysis = scraper.analyze_page(online_content)\n",
    "\n",
    "        if online_analysis:\n",
    "            report = scraper.generate_report(online_analysis)\n",
    "            print(report)\n",
    "    except Exception as e:\n",
    "        print(f\"在线网页分析跳过: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "eec73ad42b540bb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在分析在线网页（示例）...\n",
      "✅ 成功获取网页: https://books.toscrape.com/\n",
      "   状态码: 200\n",
      "   编码: utf-8\n",
      "   内容长度: 51274 字符\n",
      "=== 网页分析报告 ===\n",
      "网页标题: \n",
      "    All products | Books to Scrape - Sandbox\n",
      "\n",
      "文本总长度: 8851 字符\n",
      "HTML标签总数: 541 个\n",
      "\n",
      "--- 标题统计 ---\n",
      "H1: 1 个\n",
      "H2: 0 个\n",
      "H3: 20 个\n",
      "H4: 0 个\n",
      "H5: 0 个\n",
      "H6: 0 个\n",
      "\n",
      "--- 前5个链接 ---\n",
      "1. Books to Scrape -> index.html\n",
      "2. Home -> index.html\n",
      "3. Books -> catalogue/category/books_1/index.html\n",
      "4. Travel -> catalogue/category/books/travel_2/index.html\n",
      "5. Mystery -> catalogue/category/books/mystery_3/index.html\n",
      "\n",
      "--- 图片资源 ---\n",
      "1. media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\n",
      "2. media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f4a1c.jpg\n",
      "3. media/cache/3e/ef/3eef99c9d9adef34639f510662022830.jpg\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
