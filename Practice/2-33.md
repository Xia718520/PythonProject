机房实践报告

| 上课学院 | 资源环境学院        | 课程名称 | 地理大数据分析与应用      |
| ---- | ------------- | ---- | --------------- |
| 上课地点 | 实训大楼 218      | 机位号  | (11)号机位         |
| 专业年级 | 地理信息科学 2301 班 | 指导老师 | 李中元             |
| 学生姓名 | 夏正印           | 学生学号 | 202331108031025 |

![](C:\Users\27241\Pictures\Screenshots\屏幕截图%202025-10-15%20163921.png)

```
import requests

def get_web_info(url):
    """
    获取在线网页信息的通用函数
    """
    try:
        # 向目标网页发出请求
        response = requests.get(url)

        # 检查请求状态
        print(f"状态码: {response.status_code}")

        # 自动检测并设置编码
        response.encoding = response.apparent_encoding

        # 返回网页内容
        return response.text

    except Exception as e:
        print(f"请求失败: {e}")
        return None

# 使用示例
url = 'http://www.weather.com.cn/'
html_text = get_web_info(url)

if html_text:
    print("网页获取成功！")
    print(f"网页内容长度: {len(html_text)} 字符")
    # 打印前500个字符预览
    print(html_text[:500])
else:
    print("网页获取失败！")
```

```
import requests

def get_web_info(url):
    """
    获取在线网页信息的通用函数
    """
    try:
        # 向目标网页发出请求
        response = requests.get(url)

        # 检查请求状态
        print(f"状态码: {response.status_code}")

        # 自动检测并设置编码
        response.encoding = response.apparent_encoding

        # 返回网页内容
        return response.text

    except Exception as e:
        print(f"请求失败: {e}")
        return None

# 使用示例
url = 'http://www.weather.com.cn/'
html_text = get_web_info(url)

if html_text:
    print("网页获取成功！")
    print(f"网页内容长度: {len(html_text)} 字符")
    # 打印前500个字符预览
    print(html_text[:500])
else:
    print("网页获取失败！")
```

![](C:\Users\27241\Pictures\Screenshots\屏幕截图%202025-10-15%20165104.png)

```
def get_web_info_enhanced(url, timeout=10, headers=None):
    """
    增强版网页信息获取函数
    """
    # 默认请求头，模拟浏览器访问
    if headers is None:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    try:
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()  # 如果状态码不是200，抛出异常

        response.encoding = response.apparent_encoding
        print(f"✅ 成功获取网页: {url}")
        print(f"   状态码: {response.status_code}")
        print(f"   编码: {response.encoding}")
        print(f"   内容长度: {len(response.text)} 字符")

        return response.text

    except requests.exceptions.RequestException as e:
        print(f"❌ 请求失败: {e}")
        return None

# 测试增强版函数
test_url = 'https://httpbin.org/json'
html_content = get_web_info_enhanced(test_url)
```

```
def get_local_info(file_path):
    """
    获取本地网页信息的函数
    """
    try:
        # 打开指定的本地网页文件
        with open(file_path, 'r', encoding='utf-8') as file:
            html_content = file.read()

        print(f"✅ 成功读取本地文件: {file_path}")
        print(f"   文件内容长度: {len(html_content)} 字符")

        return html_content

    except FileNotFoundError:
        print(f"❌ 文件未找到: {file_path}")
        return None
    except Exception as e:
        print(f"❌ 读取文件失败: {e}")
        return None

# 使用示例
local_file = 'D:/PythonProject/Practice/html/index.html'
local_html = get_local_info(local_file)

if local_html:
    print("本地文件内容预览:")
    print(local_html)  # 预览
```

![](C:\Users\27241\Pictures\Screenshots\屏幕截图%202025-10-15%20165817.png)

```
class WebScraper:
    """
    通用网页采集分析类
    """

    def __init__(self):
        self.session = requests.Session()
        # 设置默认请求头
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def get_web_content(self, url, is_local=False):
        """
        获取网页内容
        """
        if is_local:
            return get_local_info(url)
        else:
            return get_web_info_enhanced(url)

    def analyze_page(self, html_content):
        """
        分析网页结构
        """
        if not html_content:
            return None

        soup = BeautifulSoup(html_content, 'html.parser')

        analysis = {
            'title': soup.title.string if soup.title else '无标题',
            'headings': {},
            'links': [],
            'images': [],
            'text_length': len(soup.get_text()),
            'tag_count': len(soup.find_all())
        }

        # 统计各级标题
        for i in range(1, 7):
            headings = soup.find_all(f'h{i}')
            analysis['headings'][f'h{i}'] = len(headings)

        # 提取链接
        links = soup.find_all('a', href=True)
        for link in links[:10]:  # 只取前10个链接
            analysis['links'].append({
                'text': link.get_text(strip=True),
                'url': link['href']
            })

        # 提取图片
        images = soup.find_all('img')
        for img in images[:5]:  # 只取前5个图片
            analysis['images'].append(img.get('src', '无src'))

        return analysis

    def generate_report(self, analysis):
        """
        生成分析报告
        """
        if not analysis:
            return "无分析数据"

        report = []
        report.append("=== 网页分析报告 ===")
        report.append(f"网页标题: {analysis['title']}")
        report.append(f"文本总长度: {analysis['text_length']} 字符")
        report.append(f"HTML标签总数: {analysis['tag_count']} 个")

        report.append("\n--- 标题统计 ---")
        for level, count in analysis['headings'].items():
            report.append(f"{level.upper()}: {count} 个")

        report.append("\n--- 前5个链接 ---")
        for i, link in enumerate(analysis['links'][:5], 1):
            report.append(f"{i}. {link['text']} -> {link['url']}")

        report.append("\n--- 图片资源 ---")
        for i, img in enumerate(analysis['images'][:3], 1):
            report.append(f"{i}. {img}")

        return "\n".join(report)

# 使用示例
def main():
    scraper = WebScraper()

    # # 分析本地网页
    # print("正在分析本地网页...")
    # local_content = scraper.get_web_content('html/index.html', is_local=True)
    # local_analysis = scraper.analyze_page(local_content)

    # if local_analysis:
    #     report = scraper.generate_report(local_analysis)
    #     print(report)

    # print("\n" + "="*60 + "\n")

    # 尝试分析在线网页（可选）
    try:
        print("正在分析在线网页（示例）...")
        online_content = scraper.get_web_content('https://books.toscrape.com/')
        online_analysis = scraper.analyze_page(online_content)

        if online_analysis:
            report = scraper.generate_report(online_analysis)
            print(report)
    except Exception as e:
        print(f"在线网页分析跳过: {e}")

if __name__ == "__main__":
    main()
```

![](C:\Users\27241\Pictures\Screenshots\屏幕截图%202025-10-15%20170046.png)

```
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

class TestScraper:
    """
    测试网站爬虫类
    """

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def test_httpbin(self):
        """测试httpbin.org"""
        print("=== 测试 httpbin.org ===")

        tests = {
            'JSON响应': 'https://httpbin.org/json',
            'HTML响应': 'https://httpbin.org/html',
            '请求头信息': 'https://httpbin.org/headers',
            'IP信息': 'https://httpbin.org/ip'
        }

        results = []
        for name, url in tests.items():
            try:
                response = self.session.get(url)
                data = {
                    '测试项目': name,
                    'URL': url,
                    '状态码': response.status_code,
                    '内容类型': response.headers.get('content-type', ''),
                    '数据大小': len(response.text)
                }
                results.append(data)
                print(f"✅ {name}: 成功")

                # 显示部分内容
                if name == 'JSON响应':
                    print(f"   示例数据: {response.json()[:100]}...")

            except Exception as e:
                print(f"❌ {name}: 失败 - {e}")

        return pd.DataFrame(results)

    def test_quotes(self):
        """测试名言网站"""
        print("\n=== 测试 quotes.toscrape.com ===")

        base_url = 'http://quotes.toscrape.com'
        quotes_data = []

        try:
            # 爬取第一页
            response = self.session.get(base_url)
            soup = BeautifulSoup(response.text, 'html.parser')

            quotes = soup.find_all('div', class_='quote')
            for quote in quotes:
                text = quote.find('span', class_='text').get_text()
                author = quote.find('small', class_='author').get_text()
                tags = [tag.get_text() for tag in quote.find_all('a', class_='tag')]

                quotes_data.append({
                    '名言': text,
                    '作者': author,
                    '标签': ', '.join(tags)
                })

            print(f"✅ 成功提取 {len(quotes_data)} 条名言")

            # 尝试翻页
            next_button = soup.find('li', class_='next')
            if next_button:
                next_page = next_button.find('a')['href']
                print(f"   发现下一页: {next_page}")

        except Exception as e:
            print(f"❌ 爬取失败: {e}")

        return pd.DataFrame(quotes_data)

    def test_books(self):
        """测试图书网站（只爬取第一页作为示例）"""
        print("\n=== 测试 books.toscrape.com ===")

        base_url = 'http://books.toscrape.com'
        books_data = []

        try:
            response = self.session.get(base_url)
            soup = BeautifulSoup(response.text, 'html.parser')

            books = soup.find_all('article', class_='product_pod')
            for book in books:
                title = book.find('h3').find('a')['title']
                price = book.find('p', class_='price_color').get_text()
                availability = book.find('p', class_='instock').get_text().strip()

                books_data.append({
                    '书名': title,
                    '价格': price,
                    '库存状态': availability
                })

            print(f"✅ 成功提取 {len(books_data)} 本图书信息")

        except Exception as e:
            print(f"❌ 爬取失败: {e}")

        return pd.DataFrame(books_data)

    def test_api_data(self):
        """测试API数据获取"""
        print("\n=== 测试 API 数据获取 ===")

        try:
            # 测试假商店API
            response = self.session.get('https://fakestoreapi.com/products')
            products = response.json()

            api_data = []
            for product in products[:5]:  # 只取前5个作为示例
                api_data.append({
                    '商品ID': product['id'],
                    '标题': product['title'],
                    '价格': product['price'],
                    '分类': product['category']
                })

            print(f"✅ 成功获取 {len(products)} 个商品数据（显示前5个）")
            return pd.DataFrame(api_data)

        except Exception as e:
            print(f"❌ API测试失败: {e}")
            return pd.DataFrame()

def main():
    """主测试函数"""
    scraper = TestScraper()

    # 执行各项测试
    httpbin_results = scraper.test_httpbin()
    quotes_results = scraper.test_quotes()
    books_results = scraper.test_books()
    api_results = scraper.test_api_data()

    # 显示结果摘要
    print("\n" + "="*50)
    print("测试结果摘要:")
    print(f"HTTPBIN测试: {len(httpbin_results)} 项")
    print(f"名言采集: {len(quotes_results)} 条")
    print(f"图书采集: {len(books_results)} 本")
    print(f"API数据: {len(api_results)} 条")

    # 保存结果（可选）
    if not quotes_results.empty:
        quotes_results.to_csv('quotes_data.csv', index=False, encoding='utf-8-sig')
        print("名言数据已保存为 quotes_data.csv")

    if not books_results.empty:
        books_results.to_csv('books_data.csv', index=False, encoding='utf-8-sig')
        print("图书数据已保存为 books_data.csv")

if __name__ == "__main__":
    main()
```

![](C:\Users\27241\Pictures\Screenshots\屏幕截图%202025-10-15%20170135.png)

![](C:\Users\27241\Pictures\Screenshots\屏幕截图%202025-10-15%20170202.png)

![](C:\Users\27241\Pictures\Screenshots\屏幕截图%202025-10-15%20170221.png)

```
import requests
from bs4 import BeautifulSoup
import csv

# 1. 目标URL
url = 'https://movie.douban.com/top250'

# 2. 设置请求头，模拟浏览器访问，避免被反爬
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# 3. 发送HTTP GET请求
response = requests.get(url, headers=headers)

# 4. 检查请求是否成功
if response.status_code == 200:
    # 5. 解析HTML内容
    soup = BeautifulSoup(response.text, 'html.parser')

    # 6. 查找所有包含电影信息的div
    movie_list = soup.find_all('div', class_='info')

    # 7. 准备一个列表来存储数据
    movies = []

    # 8. 遍历每个电影区块，提取具体信息
    for movie in movie_list:
        # 提取电影标题（在<span class="title">里）
        title = movie.find('span', class_='title').get_text()
        # 提取评分（在<span class="rating_num">里）
        rating = movie.find('span', class_='rating_num').get_text()

        # 将数据存入列表
        movies.append([title, rating])
        # 打印到控制台
        print(f"电影：{title}， 评分：{rating}")

    # 9. （可选）保存到CSV文件
    with open('douban_top250.csv', 'w', newline='', encoding='utf-8-sig') as file:
        writer = csv.writer(file)
        writer.writerow(['电影标题', '评分']) # 写入表头
        writer.writerows(movies) # 写入数据

    print("数据已保存到 douban_top250.csv")

else:
    print(f"请求失败，状态码：{response.status_code}")
```

![](C:\Users\27241\Pictures\Screenshots\屏幕截图%202025-10-15%20170259.png)
